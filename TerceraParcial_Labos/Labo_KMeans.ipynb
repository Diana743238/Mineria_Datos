{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejTVjmFOKJT5"
      },
      "source": [
        "## üî∞Laboratorio Mineria de Datos\n",
        "## üîîK-MEANS\n",
        "\n",
        "**üôã‚Äç‚ôÄÔ∏è  ALUMNA** : LEYDI DIANA CHOQUE SARMIENTO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0-oCLhxKN3j",
        "outputId": "fba93c7e-36b1-4882-fc52-2dcdb4c5da62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark==3.0.1 in /usr/local/lib/python3.7/dist-packages (3.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.7/dist-packages (0.10.9)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark==3.0.1 py4j==0.10.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nMDelleGKVFM"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP3jFHtK72eH"
      },
      "source": [
        "# **Pr√°ctica de laboratorio 5b: k-Means para cuantificar atributos**\n",
        "\n",
        "#### Los algoritmos de agrupaci√≥n de datos, adem√°s de utilizarse en el an√°lisis exploratorio para extraer patrones de similitud entre objetos, pueden utilizarse para comprimir el espacio de datos.\n",
        "\n",
        "#### En este notebook usaremos nuestra base de datos Sentiment Movie Reviews para los experimentos. Primero usaremos la t√©cnica word2vec que aprende una transformaci√≥n de tokens desde una base a un vector de atributos. A continuaci√≥n, utilizaremos el algoritmo k-Means para comprimir la informaci√≥n sobre estos atributos y proyectar cada objeto en un espacio de atributos de tama√±o fijo.\n",
        "\n",
        "#### Las celdas de ejercicio comienzan con el comentario `# EJERCICIO` y los c√≥digos a completar est√°n marcados con los comentarios `<COMPLETO>`.\n",
        "\n",
        "#### **En este notebook:**\n",
        "#### *Parte 1:* Word2Vec\n",
        "#### *Parte 2:* k-Means para cuantificar atributos\n",
        "#### *Parte 3:* Aplicar un k-NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwqr_jL272ea"
      },
      "source": [
        "###üî∞ **Parte 0: Preliminares**\n",
        "\n",
        "#### Para este notebook usaremos la base de datos de rese√±as de pel√≠culas que se usar√° para el segundo proyecto.\n",
        "\n",
        "#### La base de datos tiene los campos separados por '\\t' y el siguiente formato:\n",
        "    `\"id de frase\",\"id de oraci√≥n\",\"Frase\",\"Sentimiento\"`\n",
        "\n",
        "#### Para esta pr√°ctica de laboratorio solo usaremos el campo \"Frase\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rf6Rv6lj72ec",
        "outputId": "5f9c26dd-7545-4ed2-de36-76c025ad5d17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 8528 lines\n",
            "Sample line: (101380, 'An enjoyable , if occasionally flawed , experiment .')\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def parseRDD(point):\n",
        "    \"\"\" Parser for the current dataset. It receives a data point and return\n",
        "        a sentence (third field).\n",
        "    Args:\n",
        "        point (str): input data point\n",
        "    Returns:\n",
        "        str: a string\n",
        "    \"\"\"    \n",
        "    data = point.split('\\t')\n",
        "    return (int(data[0]),data[2])\n",
        "\n",
        "def notempty(point):\n",
        "    \"\"\" Returns whether the point string is not empty\n",
        "    Args:\n",
        "        point (str): input string\n",
        "    Returns:\n",
        "        bool: True if it is not empty\n",
        "    \"\"\"   \n",
        "    return len(point[1])>0\n",
        "\n",
        "filename = os.path.join(\"Data\",\"MovieReviews2.tsv\")\n",
        "rawRDD = sc.textFile(filename,100)\n",
        "header = rawRDD.take(1)[0]\n",
        "\n",
        "dataRDD = (rawRDD\n",
        "           #.sample(False, 0.1, seed=42)\n",
        "           .filter(lambda x: x!=header)\n",
        "           .map(parseRDD)\n",
        "           .filter(notempty)\n",
        "           #.sample( False, 0.1, 42 )\n",
        "           )\n",
        "\n",
        "print ('Read {} lines'.format(dataRDD.count()))\n",
        "print ('Sample line: {}'.format(dataRDD.takeSample(False, 1)[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wmo7pEFF72ei"
      },
      "source": [
        "###üî∞ **Parte 1: Word2Vec**\n",
        "\n",
        "#### La t√©cnica [word2vec][word2vec] aprende a trav√©s de una red neuronal sem√°ntica una representaci√≥n vectorial de cada token en un corpus de tal manera que las palabras sem√°nticamente similares son similares en la representaci√≥n vectorial.\n",
        "\n",
        "#### PySpark contiene una implementaci√≥n de esta t√©cnica, para aplicarla basta con pasar un RDD en el que cada objeto representa un documento y cada documento est√° representado por una lista de tokens en el orden en que aparecen originalmente en el corpus. Despu√©s del proceso de entrenamiento, podemos transformar un token usando el m√©todo [`transform`](https://spark.apache.org/docs/latest/ml-features) para convertir cada token en una representaci√≥n vectorial.\n",
        "\n",
        "#### En este punto, cada objeto en nuestra base estar√° representado por una matriz de tama√±o variable.\n",
        "\n",
        "[word2vec]: https://code.google.com/p/word2vec/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2231KtDk72ek"
      },
      "source": [
        "### **(1a) Generaci√≥n de RDD a partir de tokens**\n",
        "\n",
        "#### Use la funci√≥n de tokenizaci√≥n `tokenize` para generar un RDD `wordsRDD` que contenga listas de tokens de nuestra base de datos original."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sh0bA8n_72em",
        "outputId": "408bc646-4d29-4a00-892b-ac9742d237e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['quiet', 'introspective', 'entertaining', 'independent', 'worth', 'seeking']\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "import re\n",
        "\n",
        "split_regex = r'\\W+'\n",
        "\n",
        "stopfile = os.path.join(\"Data\",\"stopwords.txt\")\n",
        "stopwords = set((sc.textFile(stopfile)).collect())\n",
        "\n",
        "def tokenize(string):\n",
        "    \"\"\" An implementation of input string tokenization that excludes stopwords\n",
        "    Args:\n",
        "        string (str): input string\n",
        "    Returns:\n",
        "        list: a list of tokens without stopwords\n",
        "    \"\"\"\n",
        "    str_list = re.split(split_regex, string)\n",
        "    str_list = filter(lambda w: len(w)>0, map(lambda w: w.lower(), str_list))\n",
        "    return [w for w in str_list if w not in stopwords]\n",
        "\n",
        "wordsRDD = dataRDD.map(lambda x: tokenize(x[1]))\n",
        "\n",
        "print (wordsRDD.take(1)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "collapsed": true,
        "id": "yq_IKuWF72er"
      },
      "outputs": [],
      "source": [
        "# TEST Tokenize a String (1a)\n",
        "assert wordsRDD.take(1)[0]==[u'quiet', u'introspective', u'entertaining', u'independent', u'worth', u'seeking'], 'lista incorreta!'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH1PyHba72et"
      },
      "source": [
        "### **(1b) Aplicando la transformaci√≥n word2vec**\n",
        "\n",
        "#### Cree una plantilla word2vec aplicando el m√©todo `fit` al RDD creado en el ejercicio anterior.\n",
        "\n",
        "#### Para aplicar este m√©todo debes hacer un pipeline de m√©todos, primero ejecutando `Word2Vec()`, luego aplicando el m√©todo `setVectorSize()` con el tama√±o que queremos para nuestro vector (usa el tama√±o 5), seguido de ` setSeed()` para la semilla aleatoria, en caso de experimentos controlados (usaremos 42) y finalmente `fit()` con nuestro `wordsRDD` como par√°metro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZVz5auF72ev",
        "outputId": "32bdb578-6e63-4002-e39e-687367bc36a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.13553844392299652,0.03944551944732666,0.03806566819548607,0.08553558588027954,-0.02614559605717659]\n",
            "[('cgi', 0.989105761051178), ('something', 0.9889155626296997)]\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "from pyspark.mllib.feature import Word2Vec\n",
        "\n",
        "model = (Word2Vec()\n",
        "         .setVectorSize(5)\n",
        "         .setSeed(42)\n",
        "         .fit(wordsRDD))\n",
        "\n",
        "print (model.transform(u'entertaining'))\n",
        "print (list(model.findSynonyms(u'entertaining', 2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "collapsed": true,
        "id": "ju-ZmUeV72ex"
      },
      "outputs": [],
      "source": [
        "dist = np.abs(model.transform(u'entertaining')-np.array([-0.13553844392299652,0.03944551944732666,0.03806566819548607,0.08553558588027954,-0.02614559605717659])).mean()\n",
        "assert dist<1e-6, 'valores incorretos'\n",
        "assert list(model.findSynonyms(u'entertaining', 1))[0][0] == 'cgi', 'valores incorretos'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWJQBSRJ72ey"
      },
      "source": [
        "### **(1c) Generando un RDD de arreglos**\n",
        "\n",
        "#### Como primer paso, necesitamos generar un diccionario donde la clave son las palabras y el valor es el vector que representa esa palabra.\n",
        "\n",
        "#### Para esto primero generaremos una lista `uniqueWords` que contiene las palabras √∫nicas de las palabras RDD, eliminando aquellas que aparecen menos de 5 veces [$^1$](#1). A continuaci√≥n, crearemos un diccionario `w2v` donde la clave es un token y el valor es un `np.array` del arreglo transformado de ese token[$^2$](#2).\n",
        "\n",
        "#### Finalmente, creemos un RDD llamado `vectorsRDD` donde cada registro est√° representado por una matriz donde cada fila representa una palabra transformada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BVExHAZ72e0",
        "outputId": "f118ac4e-3057-486d-9d31-e1bc695ccf79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3388 tokens √∫nicos\n",
            "Vetor entertaining: [-0.13553844392299652,0.03944551944732666,0.03806566819548607,0.08553558588027954,-0.02614559605717659]\n",
            "(5, 5) (10, 5)\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "uniqueWords = (wordsRDD\n",
        "               .flatMap(lambda ws: [(w, 1) for w in ws])\n",
        "               .reduceByKey(lambda x,y: x+y)\n",
        "               .filter(lambda wf: wf[1]>=5)\n",
        "               .map(lambda wf: wf[0])\n",
        "               .collect()\n",
        "               )\n",
        "\n",
        "print ('{} tokens √∫nicos'.format(len(uniqueWords)))\n",
        "\n",
        "w2v = {}\n",
        "for w in uniqueWords:\n",
        "    w2v[w] = model.transform(w)\n",
        "w2vb = sc.broadcast(w2v)       \n",
        "print ('Vetor entertaining: {}'.format( w2v[u'entertaining']))\n",
        "\n",
        "vectorsRDD = (wordsRDD\n",
        "              .map(lambda ws: np.array([w2vb.value[w] for w in ws if w in w2vb.value]))\n",
        "             )\n",
        "recs = vectorsRDD.take(2)\n",
        "firstRec, secondRec = recs[0], recs[1]\n",
        "print (firstRec.shape, secondRec.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "collapsed": true,
        "id": "SjqZYy5s72e1"
      },
      "outputs": [],
      "source": [
        "# TEST Tokenizing the small datasets (1c)\n",
        "assert len(uniqueWords) == 3388,  'valor incorreto'\n",
        "assert np.mean(np.abs(w2v[u'entertaining']-[-0.13553844392299652,0.03944551944732666,0.03806566819548607,0.08553558588027954,-0.02614559605717659]))<1e-6,'valor incorreto'\n",
        "assert secondRec.shape == (10,5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okvGdbNg72e3"
      },
      "source": [
        "###üî∞ **Parte 2: k-Means para cuantificar atributos**\n",
        "\n",
        "#### Llegados a este punto, es f√°cil ver que no podemos aplicar nuestras t√©cnicas de aprendizaje supervisado a esta base de datos:\n",
        "\n",
        "   * #### La regresi√≥n log√≠stica requiere un vector de tama√±o fijo que represente cada objeto\n",
        "   * #### k-NN necesita una forma clara de comparar dos objetos, ¬øqu√© m√©trica de similitud debemos aplicar?\n",
        "  \n",
        "#### Para resolver esta situaci√≥n, realicemos una nueva transformaci√≥n en nuestro RDD. Primero, aprovechemos el hecho de que dos tokens con un significado similar se asignan a vectores similares para agruparlos en un solo atributo.\n",
        "\n",
        "#### Al aplicar k-Means a este conjunto de vectores, podemos crear $k$ puntos representativos y, para cada documento, generar un histograma de recuento de tokens en los cl√∫steres generados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvGWBBWQ72e5"
      },
      "source": [
        "#### **(2a) Agrupando los vectores y creando centros representativos**\n",
        "\n",
        "#### Como primer paso generaremos un RDD con los valores del diccionario `w2v`. A continuaci√≥n, aplicaremos el algoritmo `k-Means` con $k = 200$ y $seed = 42$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEq4uFDq72e6",
        "outputId": "0f2b4d44-21e4-43ac-d77f-c7ba163ee339"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample vector: [array([-0.07269461,  0.09603201,  0.20506908, -0.03772384,  0.08151765])]\n",
            "10 first clusters allocation: [5, 136, 37, 12, 145, 66, 63, 84, 140, 66]\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "from  pyspark.mllib.clustering import KMeans\n",
        "\n",
        "vectors2RDD = sc.parallelize(np.array(list(w2v.values())),1)\n",
        "print ('Sample vector: {}'.format(vectors2RDD.take(1)))\n",
        "\n",
        "modelK = KMeans.train(vectors2RDD, 200, seed=42)\n",
        "\n",
        "clustersRDD = vectors2RDD.map(lambda x: modelK.predict(x))\n",
        "print ('10 first clusters allocation: {}'.format(clustersRDD.take(10)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "collapsed": true,
        "id": "2az8-uNI72e8"
      },
      "outputs": [],
      "source": [
        "# TEST Amazon record with the most tokens (1d)\n",
        "assert clustersRDD.take(10)==[5, 136, 37, 12, 145, 66, 63, 84, 140, 66], 'valor incorreto'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV_Xy41j72e9"
      },
      "source": [
        "#### **(2b) Transformaci√≥n de matriz de datos en vectores cuantificados**\n",
        "\n",
        "#### El siguiente paso es transformar nuestro RDD de frases en un RDD de pares (id, vector cuantificado). Para ello crearemos una funci√≥n cuantificadora que recibir√° como par√°metros el objeto, el modelo k-means, el valor de k y el diccionario word2vec.\n",
        "\n",
        "#### Para cada punto, separemos el id y apliquemos la funci√≥n `tokenize` a la cadena. Luego transformamos la lista de tokens en una matriz word2vec. Finalmente, aplicamos cada vector de esta matriz al modelo k-Means, generando un vector de tama√±o $k$ donde cada posici√≥n $i$ indica cu√°ntos tokens pertenecen al cl√∫ster $i$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ox-Zdvyn72e-",
        "outputId": "932b6a7a-8c3e-48c6-e07a-3f7949e01bc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(64, array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0.]))]\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "def quantizador(point, model, k, w2v):\n",
        "    key = point[0]\n",
        "    words = tokenize(point[1])\n",
        "    matrix = np.array( [w2v[w] for w in words if w in w2v] )\n",
        "    features = np.zeros(k)\n",
        "    for v in matrix:\n",
        "        c = model.predict(v)\n",
        "        features[c] += 1\n",
        "    return (key, features)\n",
        "    \n",
        "quantRDD = dataRDD.map(lambda x: quantizador(x, modelK, 500, w2v))\n",
        "\n",
        "print (quantRDD.take(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "collapsed": true,
        "id": "YoAfB2_j72e_"
      },
      "outputs": [],
      "source": [
        "# TEST Implement a TF function (2a)\n",
        "assert quantRDD.take(1)[0][1].sum() == 5, 'valores incorretos'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Labo_KMeans.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}